{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: https://www.space-track.org/#queryBuilder\n",
    "# 6. Using TLE data fetched for 2 specific Starlink satellites (provide NORAD IDs 58214, 58600), determine the average rate of altitude decay (km/day) during the Gannon storm (May 10-13, 2024) and compare it to a preceding quiet period (May 1-4, 2024). (numerical comparison)\n",
    "#  answer: 1. 58214: Quiet Rate = +0.0193 km/day, Storm Rate = -0.0020 km/day, 58600: Quiet Rate = +0.0129 km/day, Storm Rate = +0.0275 km/day. 2. Average Quiet Period Rate: (+0.0193 + 0.0129) / 2 = +0.0161 km/day, Average Storm Period Rate: (-0.0020 + 0.0275) / 2 = +0.0128 km/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data for NORAD IDs: 58214, 58600\n",
      "Excluding IDs 59049, 59779 (data files empty) and 59402 (data file not provided).\n",
      "\n",
      "--- Processing NORAD ID: 58214 ---\n",
      "  Reading Quiet File: ../data_local/space-track/58214.csv\n",
      "    Read 8 valid records.\n",
      "    Quiet period analysis window: 2024-05-01T10:29:14.898336 -> 2024-05-03T19:55:27.420672\n",
      "  Reading Storm File: ../data_local/space-track/58214_20250418_685825603.csv\n",
      "    Read 13 valid records.\n",
      "    Storm period analysis window: 2024-05-10T13:03:59.806080 -> 2024-05-13T14:27:30.969792\n",
      "  Result - Quiet Rate: 0.0193 km/day\n",
      "  Result - Storm Rate: -0.0020 km/day\n",
      "\n",
      "--- Processing NORAD ID: 58600 ---\n",
      "  Reading Quiet File: ../data_local/space-track/58600.csv\n",
      "    Read 11 valid records.\n",
      "    Quiet period analysis window: 2024-05-01T14:43:45.378624 -> 2024-05-04T14:31:31.900512\n",
      "  Reading Storm File: ../data_local/space-track/58600_20250418_2069291596.csv\n",
      "    Read 13 valid records.\n",
      "    Storm period analysis window: 2024-05-10T10:55:35.480064 -> 2024-05-13T20:17:48.075648\n",
      "  Result - Quiet Rate: 0.0129 km/day\n",
      "  Result - Storm Rate: 0.0275 km/day\n",
      "\n",
      "\n",
      "--- Overall Summary ---\n",
      "--------------------------------------------------------------------------------\n",
      "NORAD ID | Quiet Rate (km/day) | Storm Rate (km/day) | Ratio (Storm/Quiet)\n",
      "---------|---------------------|---------------------|--------------------\n",
      "58214    |              0.0193 |             -0.0020 |              -0.10\n",
      "58600    |              0.0129 |              0.0275 |               2.14\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Average Rates Across Analyzed Satellites ---\n",
      "Average Quiet Period Decay Rate: 0.0161 km/day (2 satellites used)\n",
      "Average Storm Period Decay Rate: 0.0128 km/day (2 satellites used)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Analyzes Starlink altitude decay rates using local CSV files from Space-Track.\n",
    "\n",
    "Compares decay during a quiet period (May 1-4, 2024) vs. a storm period\n",
    "(May 10-13, 2024) based on GP History data.\n",
    "\n",
    "Requires CSV files containing GP History data in the same directory as the script,\n",
    "named according to the 'file_mapping' dictionary below.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import math\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import os # To handle file paths\n",
    "import sys # For error output\n",
    "\n",
    "# === Constants ===\n",
    "MU_EARTH = 398600.4418  # km^3/s^2 (Standard gravitational parameter for Earth)\n",
    "MEAN_EARTH_RADIUS = 6371.0  # km (Approximate)\n",
    "SECONDS_PER_DAY = 86400.0\n",
    "\n",
    "# === Helper Functions ===\n",
    "\n",
    "def calculate_altitude_from_mean_motion(mean_motion_rev_per_day):\n",
    "    \"\"\"Calculates mean altitude based on TLE mean motion (revs/day).\"\"\"\n",
    "    if mean_motion_rev_per_day is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Ensure input is float\n",
    "        mean_motion_rev_per_day = float(mean_motion_rev_per_day)\n",
    "        if mean_motion_rev_per_day <= 0:\n",
    "            return None\n",
    "\n",
    "        mean_motion_rad_per_sec = mean_motion_rev_per_day * 2.0 * math.pi / SECONDS_PER_DAY\n",
    "        # Calculate semi-major axis using Kepler's Third Law: a = (mu / n^2)^(1/3)\n",
    "        semi_major_axis_km = (MU_EARTH / (mean_motion_rad_per_sec ** 2)) ** (1.0 / 3.0)\n",
    "        # Calculate altitude: h = a - R_earth\n",
    "        altitude_km = semi_major_axis_km - MEAN_EARTH_RADIUS\n",
    "        return altitude_km\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"  Warning: Error calculating altitude for mean motion {mean_motion_rev_per_day}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def parse_epoch(epoch_str):\n",
    "    \"\"\"Parses Space-Track EPOCH string into a timezone-aware datetime object.\"\"\"\n",
    "    if not epoch_str:\n",
    "        return None\n",
    "    try:\n",
    "        # Handle formats like \"YYYY-MM-DD HH:MM:SS.ffffff\" or \"YYYY-MM-DDTHH:MM:SS.ffffff\"\n",
    "        epoch_str_cleaned = epoch_str.replace(' ', 'T')\n",
    "        # Ensure there's fractional seconds part for strptime robustness if needed\n",
    "        if '.' not in epoch_str_cleaned:\n",
    "             epoch_str_cleaned += '.0'\n",
    "        # Try parsing with microseconds\n",
    "        dt_obj = datetime.strptime(epoch_str_cleaned, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "\n",
    "        # Assume UTC (Space-Track standard)\n",
    "        return dt_obj.replace(tzinfo=timezone.utc)\n",
    "    except ValueError as e:\n",
    "        print(f\"  Warning: Could not parse epoch string '{epoch_str}': {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def find_relevant_records_from_list(gp_records_list, analysis_start_dt, analysis_end_dt):\n",
    "    \"\"\"\n",
    "    Finds the earliest record >= start_dt and latest record <= end_dt from a list\n",
    "    of GP records (dictionaries read from CSV).\n",
    "    Assumes the list might not be sorted initially.\n",
    "    \"\"\"\n",
    "    if not gp_records_list:\n",
    "        return None, None\n",
    "\n",
    "    # Parse epochs and filter out records where epoch can't be parsed\n",
    "    parsed_records = []\n",
    "    for record in gp_records_list:\n",
    "        epoch_dt = parse_epoch(record.get(\"EPOCH\"))\n",
    "        if epoch_dt:\n",
    "            parsed_records.append({\"epoch_dt\": epoch_dt, \"record\": record})\n",
    "        else:\n",
    "            print(f\"  Warning: Skipping record due to unparseable epoch: {record.get('EPOCH')}\", file=sys.stderr)\n",
    "\n",
    "    if not parsed_records:\n",
    "        return None, None\n",
    "\n",
    "    # Sort records by parsed epoch time\n",
    "    parsed_records.sort(key=lambda r: r[\"epoch_dt\"])\n",
    "\n",
    "    first_record_data = None\n",
    "    last_record_data = None\n",
    "\n",
    "    # Find the first record within or just before the analysis start window\n",
    "    potential_first = None\n",
    "    for item in parsed_records:\n",
    "        if item[\"epoch_dt\"] >= analysis_start_dt:\n",
    "            first_record_data = item[\"record\"]\n",
    "            break # Found the first one at or after the start\n",
    "        potential_first = item[\"record\"] # Keep track of latest before start\n",
    "\n",
    "    if first_record_data is None:\n",
    "        first_record_data = potential_first # Use the one just before if none are after start\n",
    "\n",
    "    # Find the last record within the analysis end window\n",
    "    potential_last = None\n",
    "    for item in reversed(parsed_records): # Search backwards for efficiency\n",
    "         if item[\"epoch_dt\"] <= analysis_end_dt:\n",
    "              # Ensure this record is not before the chosen first record's epoch\n",
    "              if first_record_data:\n",
    "                   first_epoch_dt = parse_epoch(first_record_data.get(\"EPOCH\"))\n",
    "                   if first_epoch_dt and item[\"epoch_dt\"] >= first_epoch_dt:\n",
    "                        last_record_data = item[\"record\"]\n",
    "                        break # Found the last one at or before the end, and after the start\n",
    "              else: # Should not happen if potential_first logic worked, but as safety\n",
    "                   last_record_data = item[\"record\"]\n",
    "                   break\n",
    "         # Keep track of the earliest record after the end window as a fallback? No, usually want latest *within*.\n",
    "\n",
    "    # If no record found within end window, but we have a start record\n",
    "    if last_record_data is None and first_record_data is not None:\n",
    "         # Check if the very last record overall is usable (i.e. same or after first record)\n",
    "         last_available_epoch = parsed_records[-1][\"epoch_dt\"]\n",
    "         first_epoch_dt = parse_epoch(first_record_data.get(\"EPOCH\"))\n",
    "         if first_epoch_dt and last_available_epoch >= first_epoch_dt:\n",
    "              last_record_data = parsed_records[-1][\"record\"]\n",
    "              # print(\"  Info: Using last available record as end point as none were strictly within end boundary.\")\n",
    "\n",
    "\n",
    "    # Final check: Ensure we have both and they are not the same record if possible\n",
    "    if first_record_data and last_record_data:\n",
    "        # Avoid using the exact same record if multiple exist\n",
    "        if first_record_data.get(\"GP_ID\") == last_record_data.get(\"GP_ID\") and len(parsed_records) > 1:\n",
    "             # This check might be too simplistic if GP_IDs repeat; rely on epoch instead\n",
    "             first_epoch = parse_epoch(first_record_data.get(\"EPOCH\"))\n",
    "             last_epoch = parse_epoch(last_record_data.get(\"EPOCH\"))\n",
    "             if first_epoch == last_epoch and len(parsed_records) > 1:\n",
    "                  print(\"  Warning: Start and end records are identical, cannot calculate rate.\", file=sys.stderr)\n",
    "                  return None, None # Cannot calculate rate with identical points\n",
    "\n",
    "        # Check if first is actually before last\n",
    "        first_epoch = parse_epoch(first_record_data.get(\"EPOCH\"))\n",
    "        last_epoch = parse_epoch(last_record_data.get(\"EPOCH\"))\n",
    "        if not first_epoch or not last_epoch or first_epoch > last_epoch:\n",
    "             print(\"  Warning: Could not determine valid start/end epoch order.\", file=sys.stderr)\n",
    "             return None, None # Invalid state\n",
    "    elif not first_record_data or not last_record_data:\n",
    "         # If either is still None here, we don't have a valid pair\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    return first_record_data, last_record_data\n",
    "\n",
    "\n",
    "def calculate_decay_rate(start_record, end_record):\n",
    "    \"\"\"\n",
    "    Calculates the average altitude decay rate between two GP records (dictionaries).\n",
    "    Returns rate in km/day or None if calculation isn't possible.\n",
    "    \"\"\"\n",
    "    if not start_record or not end_record:\n",
    "        # This case should be handled before calling, but double-check\n",
    "        # print(\"  Calculation Error: Missing start or end record for period.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract data\n",
    "        start_epoch_str = start_record.get(\"EPOCH\")\n",
    "        start_mean_motion = start_record.get(\"MEAN_MOTION\")\n",
    "        end_epoch_str = end_record.get(\"EPOCH\")\n",
    "        end_mean_motion = end_record.get(\"MEAN_MOTION\")\n",
    "\n",
    "        # Basic checks for essential data\n",
    "        if None in [start_epoch_str, start_mean_motion, end_epoch_str, end_mean_motion]:\n",
    "            print(\"  Calculation Error: Missing EPOCH or MEAN_MOTION in records.\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        # Convert mean motion to float, handle potential errors\n",
    "        try:\n",
    "            start_mean_motion_f = float(start_mean_motion)\n",
    "            end_mean_motion_f = float(end_mean_motion)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"  Calculation Error: Invalid MEAN_MOTION value: {e}\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "\n",
    "        # Calculate altitudes\n",
    "        start_alt = calculate_altitude_from_mean_motion(start_mean_motion_f)\n",
    "        end_alt = calculate_altitude_from_mean_motion(end_mean_motion_f)\n",
    "\n",
    "        if start_alt is None or end_alt is None:\n",
    "             print(\"  Calculation Error: Could not calculate altitude from mean motion.\", file=sys.stderr)\n",
    "             return None\n",
    "\n",
    "        # Calculate time difference\n",
    "        start_time = parse_epoch(start_epoch_str)\n",
    "        end_time = parse_epoch(end_epoch_str)\n",
    "\n",
    "        if not start_time or not end_time:\n",
    "            print(\"  Calculation Error: Could not parse epoch strings.\", file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        time_delta_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "        # Prevent division by zero or negative time\n",
    "        # Allow very small positive time differences\n",
    "        if time_delta_seconds <= 1e-6: # Use a small threshold > 0\n",
    "             print(f\"  Calculation Error: Time difference too small or zero ({time_delta_seconds:.4f}s). Cannot calculate rate.\", file=sys.stderr)\n",
    "             return None\n",
    "\n",
    "        time_delta_days = time_delta_seconds / SECONDS_PER_DAY\n",
    "\n",
    "        # Calculate altitude change\n",
    "        altitude_delta_km = end_alt - start_alt # Will be negative for decay\n",
    "\n",
    "        # Calculate rate\n",
    "        decay_rate_km_per_day = altitude_delta_km / time_delta_days\n",
    "        return decay_rate_km_per_day\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors during calculation\n",
    "        print(f\"  Calculation Error: Unexpected exception: {e}\", file=sys.stderr)\n",
    "        # Optionally print records for debugging:\n",
    "        # print(f\"    Start Record causing error: {start_record}\", file=sys.stderr)\n",
    "        # print(f\"    End Record causing error: {end_record}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "# === Main Analysis Logic ===\n",
    "\n",
    "def run_analysis():\n",
    "    \"\"\"Reads local CSV files, calculates, and compares decay rates.\"\"\"\n",
    "\n",
    "    # Define where your downloaded files are relative to the script location\n",
    "    # Assumes files are in the same directory as the script.\n",
    "    data_directory = \"../data_local/space-track/\"\n",
    "\n",
    "    # Define the mapping between satellite IDs, periods, and filenames\n",
    "    # Uses the specific filenames identified from user uploads.\n",
    "    # Excludes satellites where data files were empty or not provided.\n",
    "    file_mapping = {\n",
    "        \"58214\": {\n",
    "            \"quiet\": \"58214.csv\",\n",
    "            \"storm\": \"58214_20250418_685825603.csv\"\n",
    "        },\n",
    "        \"58600\": {\n",
    "            \"quiet\": \"58600.csv\",\n",
    "            \"storm\": \"58600_20250418_2069291596.csv\"\n",
    "        },\n",
    "    }\n",
    "    analyzed_ids = list(file_mapping.keys())\n",
    "    print(f\"Analyzing data for NORAD IDs: {', '.join(analyzed_ids)}\")\n",
    "    print(\"Excluding IDs 59049, 59779 (data files empty) and 59402 (data file not provided).\")\n",
    "\n",
    "\n",
    "    # Define analysis windows (UTC)\n",
    "    ANALYSIS_QUIET_START = datetime(2024, 5, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "    ANALYSIS_QUIET_END = datetime(2024, 5, 4, 23, 59, 59, 999999, tzinfo=timezone.utc)\n",
    "    ANALYSIS_STORM_START = datetime(2024, 5, 10, 0, 0, 0, tzinfo=timezone.utc)\n",
    "    ANALYSIS_STORM_END = datetime(2024, 5, 13, 23, 59, 59, 999999, tzinfo=timezone.utc)\n",
    "\n",
    "    results = {} # Dictionary to store calculated rates {norad_id: {period: rate}}\n",
    "\n",
    "    # --- Loop through satellites and periods ---\n",
    "    for norad_id, period_files in file_mapping.items():\n",
    "        print(f\"\\n--- Processing NORAD ID: {norad_id} ---\")\n",
    "        results[norad_id] = {\"quiet_rate_km_day\": None, \"storm_rate_km_day\": None}\n",
    "\n",
    "        # --- Process Quiet Period File ---\n",
    "        quiet_filename = period_files.get(\"quiet\")\n",
    "        if quiet_filename:\n",
    "            filepath = os.path.join(data_directory, quiet_filename)\n",
    "            print(f\"  Reading Quiet File: {filepath}\")\n",
    "            try:\n",
    "                quiet_records = []\n",
    "                with open(filepath, 'r', encoding='utf-8', newline='') as f:\n",
    "                    # Check for empty file / only header\n",
    "                    first_line = f.readline()\n",
    "                    if not first_line:\n",
    "                         print(\"  Warning: Quiet file appears empty.\")\n",
    "                         continue # Skip to next period/satellite\n",
    "\n",
    "                    # Check for \"NO RESULTS RETURNED\"\n",
    "                    if \"NO RESULTS RETURNED\" in first_line:\n",
    "                         print(\"  Warning: Quiet file contains 'NO RESULTS RETURNED'.\")\n",
    "                         continue\n",
    "\n",
    "                    f.seek(0) # Rewind to read header with DictReader\n",
    "                    reader = csv.DictReader(f)\n",
    "                    # Check if header exists\n",
    "                    if not reader.fieldnames:\n",
    "                         print(f\"  Error: Could not read header from quiet file: {filepath}\", file=sys.stderr)\n",
    "                         continue\n",
    "\n",
    "                    # Check for required columns\n",
    "                    required_cols = [\"EPOCH\", \"MEAN_MOTION\"]\n",
    "                    if not all(col in reader.fieldnames for col in required_cols):\n",
    "                        print(f\"  Error: Missing required columns ({', '.join(required_cols)}) in quiet file: {filepath}\", file=sys.stderr)\n",
    "                        continue\n",
    "\n",
    "                    # Read data rows\n",
    "                    for row in reader:\n",
    "                        # Basic validation: ensure required fields are not empty strings\n",
    "                        if row.get(\"EPOCH\") and row.get(\"MEAN_MOTION\"):\n",
    "                             quiet_records.append(row)\n",
    "                        else:\n",
    "                             print(f\"  Warning: Skipping row with missing required data in quiet file: {row}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "                if quiet_records:\n",
    "                    print(f\"    Read {len(quiet_records)} valid records.\")\n",
    "                    q_start_rec, q_end_rec = find_relevant_records_from_list(\n",
    "                        quiet_records, ANALYSIS_QUIET_START, ANALYSIS_QUIET_END\n",
    "                    )\n",
    "                    if q_start_rec and q_end_rec:\n",
    "                        print(f\"    Quiet period analysis window: {q_start_rec.get('EPOCH')} -> {q_end_rec.get('EPOCH')}\")\n",
    "                        quiet_rate = calculate_decay_rate(q_start_rec, q_end_rec)\n",
    "                        results[norad_id][\"quiet_rate_km_day\"] = quiet_rate\n",
    "                        if quiet_rate is None:\n",
    "                             print(\"    Failed to calculate quiet decay rate.\")\n",
    "                    else:\n",
    "                        print(\"    Could not find suitable start/end records within the quiet analysis window.\")\n",
    "                else:\n",
    "                    print(\"    No valid records found in quiet file after reading.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  Error: Quiet file not found: {filepath}\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing quiet file {filepath}: {e}\", file=sys.stderr)\n",
    "                import traceback\n",
    "                traceback.print_exc(file=sys.stderr)\n",
    "\n",
    "\n",
    "        # --- Process Storm Period File (Similar logic) ---\n",
    "        storm_filename = period_files.get(\"storm\")\n",
    "        if storm_filename:\n",
    "            filepath = os.path.join(data_directory, storm_filename)\n",
    "            print(f\"  Reading Storm File: {filepath}\")\n",
    "            try:\n",
    "                storm_records = []\n",
    "                with open(filepath, 'r', encoding='utf-8', newline='') as f:\n",
    "                     # Check for empty file / only header\n",
    "                    first_line = f.readline()\n",
    "                    if not first_line:\n",
    "                         print(\"  Warning: Storm file appears empty.\")\n",
    "                         continue # Skip to next satellite\n",
    "\n",
    "                    # Check for \"NO RESULTS RETURNED\"\n",
    "                    if \"NO RESULTS RETURNED\" in first_line:\n",
    "                         print(\"  Warning: Storm file contains 'NO RESULTS RETURNED'.\")\n",
    "                         continue\n",
    "\n",
    "                    f.seek(0) # Rewind to read header with DictReader\n",
    "                    reader = csv.DictReader(f)\n",
    "                     # Check if header exists\n",
    "                    if not reader.fieldnames:\n",
    "                         print(f\"  Error: Could not read header from storm file: {filepath}\", file=sys.stderr)\n",
    "                         continue\n",
    "\n",
    "                    # Check for required columns\n",
    "                    required_cols = [\"EPOCH\", \"MEAN_MOTION\"]\n",
    "                    if not all(col in reader.fieldnames for col in required_cols):\n",
    "                        print(f\"  Error: Missing required columns ({', '.join(required_cols)}) in storm file: {filepath}\", file=sys.stderr)\n",
    "                        continue\n",
    "\n",
    "                    # Read data rows\n",
    "                    for row in reader:\n",
    "                         if row.get(\"EPOCH\") and row.get(\"MEAN_MOTION\"):\n",
    "                             storm_records.append(row)\n",
    "                         else:\n",
    "                             print(f\"  Warning: Skipping row with missing required data in storm file: {row}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "                if storm_records:\n",
    "                    print(f\"    Read {len(storm_records)} valid records.\")\n",
    "                    s_start_rec, s_end_rec = find_relevant_records_from_list(\n",
    "                        storm_records, ANALYSIS_STORM_START, ANALYSIS_STORM_END\n",
    "                    )\n",
    "                    if s_start_rec and s_end_rec:\n",
    "                        print(f\"    Storm period analysis window: {s_start_rec.get('EPOCH')} -> {s_end_rec.get('EPOCH')}\")\n",
    "                        storm_rate = calculate_decay_rate(s_start_rec, s_end_rec)\n",
    "                        results[norad_id][\"storm_rate_km_day\"] = storm_rate\n",
    "                        if storm_rate is None:\n",
    "                             print(\"    Failed to calculate storm decay rate.\")\n",
    "                    else:\n",
    "                        print(\"    Could not find suitable start/end records within the storm analysis window.\")\n",
    "                else:\n",
    "                    print(\"    No valid records found in storm file after reading.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  Error: Storm file not found: {filepath}\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing storm file {filepath}: {e}\", file=sys.stderr)\n",
    "                import traceback\n",
    "                traceback.print_exc(file=sys.stderr)\n",
    "\n",
    "        # --- Print individual results after processing both periods ---\n",
    "        qr = results[norad_id].get('quiet_rate_km_day')\n",
    "        sr = results[norad_id].get('storm_rate_km_day')\n",
    "        print(f\"  Result - Quiet Rate: {qr:.4f} km/day\" if qr is not None else \"  Result - Quiet Rate: Error/Unavailable\")\n",
    "        print(f\"  Result - Storm Rate: {sr:.4f} km/day\" if sr is not None else \"  Result - Storm Rate: Error/Unavailable\")\n",
    "\n",
    "    # --- Aggregate and Summarize Results ---\n",
    "    total_quiet_rate = 0\n",
    "    valid_quiet_count = 0\n",
    "    total_storm_rate = 0\n",
    "    valid_storm_count = 0\n",
    "\n",
    "    print(\"\\n\\n--- Overall Summary ---\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(\"NORAD ID | Quiet Rate (km/day) | Storm Rate (km/day) | Ratio (Storm/Quiet)\")\n",
    "    print(\"---------|---------------------|---------------------|--------------------\")\n",
    "\n",
    "    # Ensure consistent iteration order for printing\n",
    "    sorted_ids = sorted(results.keys())\n",
    "\n",
    "    for norad_id in sorted_ids:\n",
    "        rates = results[norad_id]\n",
    "        qr = rates.get('quiet_rate_km_day')\n",
    "        sr = rates.get('storm_rate_km_day')\n",
    "        ratio_str = \"N/A\"\n",
    "\n",
    "        if qr is not None:\n",
    "            total_quiet_rate += qr\n",
    "            valid_quiet_count += 1\n",
    "        if sr is not None:\n",
    "            total_storm_rate += sr\n",
    "            valid_storm_count += 1\n",
    "\n",
    "        # Calculate ratio only if both rates are valid and quiet rate isn't effectively zero\n",
    "        if qr is not None and sr is not None and abs(qr) > 1e-9:\n",
    "            ratio_str = f\"{sr / qr:.2f}\"\n",
    "        elif qr is None or sr is None:\n",
    "            ratio_str = \"N/A\" # One or both rates unavailable\n",
    "        else: # qr is near zero\n",
    "            ratio_str = \"Inf\" if sr !=0 else \"NaN\" # Handle division by zero case\n",
    "\n",
    "\n",
    "        qr_str = f\"{qr: >19.4f}\" if qr is not None else f\"{'Error/Unavailable': >19}\"\n",
    "        sr_str = f\"{sr: >19.4f}\" if sr is not None else f\"{'Error/Unavailable': >19}\"\n",
    "\n",
    "        print(f\"{norad_id:<8} | {qr_str} | {sr_str} | {ratio_str:>18}\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    # --- Calculate Averages ---\n",
    "    avg_quiet_rate = total_quiet_rate / valid_quiet_count if valid_quiet_count > 0 else None\n",
    "    avg_storm_rate = total_storm_rate / valid_storm_count if valid_storm_count > 0 else None\n",
    "\n",
    "    print(\"\\n--- Average Rates Across Analyzed Satellites ---\")\n",
    "    if avg_quiet_rate is not None:\n",
    "         print(f\"Average Quiet Period Decay Rate: {avg_quiet_rate:.4f} km/day ({valid_quiet_count} satellites used)\")\n",
    "    else:\n",
    "         print(f\"Average Quiet Period Decay Rate: Could not be calculated ({valid_quiet_count} satellites valid).\")\n",
    "\n",
    "    if avg_storm_rate is not None:\n",
    "         print(f\"Average Storm Period Decay Rate: {avg_storm_rate:.4f} km/day ({valid_storm_count} satellites used)\")\n",
    "    else:\n",
    "         print(f\"Average Storm Period Decay Rate: Could not be calculated ({valid_storm_count} satellites valid).\")\n",
    "\n",
    "# === Main execution block ===\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
